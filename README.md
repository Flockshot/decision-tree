# ID3 Decision Tree Classifier Implementation

Developed a complete Decision Tree classifier from scratch using the ID3 algorithm, applying concepts from information theory to implement a fully functional, interpretable, rule-based machine learning model.

This project was built entirely in Python without relying on external machine learning libraries (like `scikit-learn`) to demonstrate a ground-up understanding of the algorithm.

![Python](https://img.shields.io/badge/Python-3.x-blue.svg?logo=python&logoColor=white)
![Algorithm](https://img.shields.io/badge/Algorithm-ID3-orange.svg)

---

## ðŸš€ Core Features

* **From-Scratch Implementation:** The entire classifier, including all supporting logic, is built from the ground up in the `ID3.py` file.
* **Information Theory Engine:** Implements the key metrics that power the ID3 algorithm:
    * **Entropy:** Measures the impurity or "disorder" of a dataset.
    * **Information Gain:** Calculates the reduction in entropy achieved by splitting the data on a specific attribute.
    * **Gain Ratio:** An improvement on Information Gain that normalizes by "Split Info," preventing bias towards attributes with many unique values (like an ID column).
* **Recursive Tree Construction:** A core `build_tree` function recursively selects the best attribute for splitting, partitions the dataset, and builds the tree until a stopping criterion (e.g., a "pure" node) is met.
* **Custom Tree Structure:** Uses custom `TreeNode` and `TreeLeafNode` classes to represent the hierarchical decisions and the final classification outcomes.
* **Selectable Splitting Criteria:** The model can be run using either standard **Information Gain** or the more robust **Gain Ratio** as the attribute selection metric.

---

## ðŸ¤– How the Algorithm Works

The `ID3.py` script follows the classic ID3 (Iterative Dichotomiser 3) workflow:

1.  **Calculate Base Entropy:** First, the entropy of the entire dataset (the "root" node) is calculated based on the distribution of the target class labels.
2.  **Evaluate All Attributes:** For every attribute (e.g., "Outlook", "Temperature"):
    * The dataset is split into subsets, one for each unique value of that attribute (e.g., "Sunny", "Overcast", "Rainy").
    * The entropy for *each subset* is calculated.
    * A weighted average of these child entropies is computed.
    * The **Information Gain** is calculated: `Entropy(Parent) - WeightedAverageEntropy(Children)`.
    * (Optional) The **Gain Ratio** is calculated to normalize the Information Gain.
3.  **Select Best Attribute:** The attribute with the **highest Information Gain (or Gain Ratio)** is chosen as the "best" splitting attribute for the current node.
4.  **Build Tree Recursively:**
    * A `TreeNode` is created for the selected attribute.
    * For each unique value of that attribute, a new branch is created.
    * The `build_tree` function is called recursively on the corresponding subset of data for that branch.
5.  **Stopping Conditions (Leaf Nodes):** The recursion stops and a `TreeLeafNode` is created when:
    * A node's subset of data is "pure" (i.e., all remaining samples belong to the same class).
    * There are no more attributes left to split on.
6.  **Classification:** To classify a new, unseen sample, the `mainID3.py` script traverses the tree from the root, following the branches that match the sample's attribute values until it reaches a `TreeLeafNode`, which provides the predicted class.

### Example Tree Output

> **[Image: A simple decision tree generated by the model]**
>
> *(**Developer Note:** Place a text or graphical representation of a simple tree your model generates here. For example:*
>
> ```
> <Outlook>
> â”œâ”€â”€ Sunny
> â”‚   â””â”€â”€ <Humidity>
> â”‚       â”œâ”€â”€ High: No
> â”‚       â””â”€â”€ Normal: Yes
> â”œâ”€â”€ Overcast: Yes
> â”œâ”€â”€ Rainy
> â”‚   â””â”€â”€ <Windy>
> â”‚       â”œâ”€â”€ True: No
> â”‚       â””â”€â”€ False: Yes
> ```
> *)*

---

## ðŸ“‚ Project Structure

* **`mainID3.py`:** The main executable script. This file is responsible for loading a dataset (not included in repo), initiating the tree-building process from `ID3.py`, and running the classifier to make predictions.
* **`ID3.py`:** The core library containing all the logic:
    * `TreeNode` and `TreeLeafNode` classes.
    * Functions for calculating `entropy`, `information_gain`, and `gain_ratio`.
    * The main recursive `build_tree` function.
    * The `classify` function for tree traversal.

---

## ðŸš€ How to Run

1.  Clone the repository.
2.  Provide a categorical dataset (e.g., `tennis.csv`) in the format expected by `mainID3.py`.
3.  Run the main script from your terminal:

    ```bash
    python mainID3.py
    ```